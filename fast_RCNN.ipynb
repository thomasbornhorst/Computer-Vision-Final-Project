{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster RCNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.optim import SGD\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bb_left and bb_top are the top left corner, and bb_width and bb_height are the dimensions of the bounding box\n",
    "x_max would be bb_left + bb_width\n",
    "y_max would be bb_top + bb_height\n",
    "x_min = bb_left\n",
    "y_min = bb_top\n",
    "Aim to track the player id in each frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_annotations(csv_path):\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def extract_frames(video_path, output_dir, frame_numbers):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    success, frame_idx = True, 0\n",
    "    while success:\n",
    "        success, frame = cap.read()\n",
    "        if frame_idx in frame_numbers:\n",
    "            frame_path = os.path.join(output_dir, f\"frame_{frame_idx:04d}.jpg\")\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "        frame_idx += 1\n",
    "    cap.release()\n",
    "\n",
    "def get_frame_path(frame_number, output_dir):\n",
    "    return os.path.join(output_dir, f\"frame_{frame_number:04d}.jpg\")\n",
    "\n",
    "def preprocess_annotations(annotations, output_dir):\n",
    "    annotations['x_max'] = annotations['bb_left'] + annotations['bb_width']\n",
    "    annotations['y_max'] = annotations['bb_top'] + annotations['bb_height']\n",
    "    annotations['frame_path'] = annotations['frame'].apply(lambda x: get_frame_path(x, output_dir))\n",
    "    return annotations\n",
    "\n",
    "def split_dataset(annotations, val_size=0.2, test_size=0.1):\n",
    "    initial_train_and_val_size = 1 - test_size\n",
    "    train_val_annots, test_annots = train_test_split(annotations, test_size=test_size)\n",
    "    adjusted_val_size = val_size / initial_train_and_val_size\n",
    "    train_annots, val_annots = train_test_split(train_val_annots, test_size=adjusted_val_size)\n",
    "    return train_annots, val_annots, test_annots\n",
    "\n",
    "def main(annotations_dir, videos_dir, frames_output_dir):\n",
    "    all_annotations = []\n",
    "    \n",
    "    for i in range(60):  # Assuming 60 pairs of videos and CSV files (top view dataset)\n",
    "        csv_path = os.path.join(annotations_dir, f\"D_20220220_1_{i:04d}_0030.csv\")\n",
    "        video_path = os.path.join(videos_dir, f\"D_20220220_1_{i:04d}_0030.mp4\")\n",
    "        \n",
    "        annotations = load_annotations(csv_path)\n",
    "        frame_numbers = sorted(annotations['frame'].unique())\n",
    "        \n",
    "        extract_frames(video_path, frames_output_dir, frame_numbers)\n",
    "        \n",
    "        preprocessed_annotations = preprocess_annotations(annotations, frames_output_dir)\n",
    "        all_annotations.append(preprocessed_annotations)\n",
    "    \n",
    "    all_annotations_df = pd.concat(all_annotations, ignore_index=True)\n",
    "    \n",
    "    train_annots, val_annots, test_annots = split_dataset(all_annotations_df, val_size=0.2, test_size=0.1)\n",
    "\n",
    "    #  use train_annots, val_annots, and test_annots for training, validating, and testing the model\n",
    "    print(\"Training annotations:\", len(train_annots))\n",
    "    print(\"Validation annotations:\", len(val_annots))\n",
    "    print(\"Testing annotations:\", len(test_annots))\n",
    "\n",
    "# Example usage\n",
    "annotations_dir = '/path/to/annotations/'\n",
    "videos_dir = '/path/to/videos/'\n",
    "frames_output_dir = '/path/to/extracted_frames/'\n",
    "main(annotations_dir, videos_dir, frames_output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoccerTrackDataset(Dataset):\n",
    "    def __init__(self, annotations, img_dir, transforms=None):\n",
    "        self.annotations = annotations\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.annotations.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, row['frame_path'])\n",
    "        image = read_image(img_path).float() / 255.0  # Normalize to [0, 1]\n",
    "        \n",
    "        # Increment player ID by 1 to reserve 0 for the background\n",
    "        box = torch.tensor([row['bb_left'], row['bb_top'], row['x_max'], row['y_max']], dtype=torch.float32).unsqueeze(0)\n",
    "        label = row['PlayerID'] + 1  # Increment player ID by 1\n",
    "        labels = torch.tensor([label], dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = box\n",
    "        target['labels'] = labels\n",
    "        target['image_id'] = torch.tensor([idx])\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        \n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_faster_rcnn_model(num_classes):\n",
    "    \"\"\"\n",
    "    Load a pre-trained Faster R-CNN model and replace the classifier head with one\n",
    "    that has `num_classes`, accounting for the background and player IDs.\n",
    "\n",
    "    Parameters:\n",
    "    - num_classes (int): The total number of classes including the background.\n",
    "\n",
    "    Returns:\n",
    "    - model (FasterRCNN): A Faster R-CNN model adjusted for the specified number of classes.\n",
    "    \"\"\"\n",
    "    # Load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    # Get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Replace the pre-trained head with a new one for the specified number of classes\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, optimizer, device, num_epochs=10000):\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for images, targets in data_loader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += losses.item()\n",
    "        \n",
    "        print(f\"Epoch #{epoch+1} Loss: {total_loss / len(data_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    annotations_path = '/path/to/your/annotations.csv'\n",
    "    img_dir = '/path/to/your/images'\n",
    "    annotations_df = pd.read_csv(annotations_path)\n",
    "    max_player_id = annotations_df['PlayerID'].max()\n",
    "\n",
    "    # Increment max_player_id by 1 since we shifted player IDs by +1 to reserve 0 for background\n",
    "    model = get_faster_rcnn_model(max_player_id + 2)\n",
    "\n",
    "    dataset = SoccerTrackDataset(annotations_df, img_dir, transforms=T.ToTensor())\n",
    "    data_loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=lambda x: list(zip(*x)))\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.005)\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    train_model(model, data_loader, optimizer, device, num_epochs=10)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When interpreting the model's predictions, remember that the player ID predictions are shifted by +1. To match predictions with the original player IDs, we need to decrement the predicted class IDs by 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
